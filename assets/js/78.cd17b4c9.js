(window.webpackJsonp=window.webpackJsonp||[]).push([[78],{444:function(_,t,v){"use strict";v.r(t);var a=v(3),l=Object(a.a)({},(function(){var _=this,t=_._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[t("h1",{attrs:{id:"机器学习基础"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#机器学习基础"}},[_._v("#")]),_._v(" 机器学习基础")]),_._v(" "),t("ClientOnly",[t("title-pv")],1),_._v(" "),t("h2",{attrs:{id:"决策树"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#决策树"}},[_._v("#")]),_._v(" 决策树")]),_._v(" "),t("p",[_._v("普通决策树（如CART决策树）和基于梯度的决策树（如梯度提升决策树，GBDT）的主要区别体现在模型构建方式和目标优化方法上。下面从公式层面来分析两者的差异。")]),_._v(" "),t("h2",{attrs:{id:"普通决策树-cart"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#普通决策树-cart"}},[_._v("#")]),_._v(" 普通决策树（CART）")]),_._v(" "),t("p",[_._v("普通决策树是通过递归地对数据进行划分，基于某种标准（如基尼系数或信息增益）选择特征和分裂点构建的。其目标是最大化子节点的纯度。")]),_._v(" "),t("p",[t("strong",[_._v("决策过程：")])]),_._v(" "),t("ul",[t("li",[t("p",[_._v("对于每个节点，决策树尝试找到最优特征 "),t("smalltex",[_._v(" j ")]),_._v(" 和分裂点 "),t("smalltex",[_._v(" s ")]),_._v("，使得分裂后的损失函数（如基尼系数或熵）最小化。")],1),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\n\\min_{j, s} \\left( L(\\text{左子节点}) + L(\\text{右子节点}) \\right)\n")])],1)])]),_._v(" "),t("p",[_._v("其中，损失函数 "),t("smalltex",[_._v(" L ")]),_._v(" 可能是基尼系数 "),t("smalltex",[_._v(" G ")]),_._v(" 或熵 "),t("smalltex",[_._v(" H ")]),_._v("：")],1),_._v(" "),t("ul",[t("li",[_._v("基尼系数："),t("smalltex",[_._v(" G(p) = 1 - \\sum_{i=1}^{k} p_i^2 ")])],1),_._v(" "),t("li",[_._v("熵："),t("smalltex",[_._v(" H(p) = -\\sum_{i=1}^{k} p_i \\log(p_i) ")])],1)]),_._v(" "),t("p",[_._v("树的构建是贪心的，每次选取当前最优分裂点。")]),_._v(" "),t("h2",{attrs:{id:"回归树"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#回归树"}},[_._v("#")]),_._v(" 回归树")]),_._v(" "),t("h2",{attrs:{id:"随机森林"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#随机森林"}},[_._v("#")]),_._v(" 随机森林")]),_._v(" "),t("h2",{attrs:{id:"梯度决策树-gbdt"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#梯度决策树-gbdt"}},[_._v("#")]),_._v(" 梯度决策树（GBDT）")]),_._v(" "),t("p",[_._v("GBDT 是一种集成模型，通过不断迭代训练多个决策树来优化模型性能。与普通决策树不同，GBDT的每棵树是在前一棵树的预测残差（误差）的基础上构建的，目的是通过迭代地最小化残差来提高预测精度。")]),_._v(" "),t("p",[_._v("需要注意的是，GBDT并不是真正的用决策树预测结果，而是用决策树作为梯度指导去优化一个模型，使模型能达到更好的性能。")]),_._v(" "),t("p",[_._v("GBDT的目标是最小化整体损失函数：")]),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nL = \\sum_{i=1}^{n} \\ell(y_i, F_M(x_i))\n")])],1),_._v(" "),t("p",[_._v("其中：")]),_._v(" "),t("ul",[t("li",[t("smalltex",[_._v(" y_i ")]),_._v(" 是实际标签，"),t("smalltex",[_._v(" F_M(x_i) ")]),_._v(" 是第 "),t("smalltex",[_._v(" M ")]),_._v(" 轮模型的预测值。")],1),_._v(" "),t("li",[t("smalltex",[_._v(" \\ell(y_i, F(x_i)) ")]),_._v(" 是损失函数，常见的是均方误差（MSE）："),t("smalltex",[_._v(" \\ell(y_i, F(x_i)) = (y_i - F(x_i))^2 ")]),_._v("。")],1),_._v(" "),t("li",[t("smalltex",[_._v(" F_M(x_i) = F_{M-1}(x_i) + \\eta \\cdot h_M(x_i) ")]),_._v("，其中 "),t("smalltex",[_._v(" h_M(x_i) ")]),_._v(" 是第 "),t("smalltex",[_._v(" M ")]),_._v(" 棵树，"),t("smalltex",[_._v(" \\eta ")]),_._v(" 是学习率。")],1)]),_._v(" "),t("p",[t("strong",[_._v("梯度提升步骤：")])]),_._v(" "),t("ol",[t("li",[t("p",[t("strong",[_._v("初始化模型")]),_._v("：首先初始化一个常数模型，通常为目标变量的均值。")]),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nF_0(x) = \\arg\\min_{c} \\sum_{i=1}^{n} \\ell(y_i, c)\n")])],1)]),_._v(" "),t("li",[t("p",[t("strong",[_._v("计算残差")]),_._v("：在第 "),t("smalltex",[_._v(" M ")]),_._v(" 步，计算前一轮模型的残差（即当前的负梯度）：")],1),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nr_i^{(M)} = - \\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x_i) = F_{M-1}(x_i)}\n")])],1)]),_._v(" "),t("li",[t("p",[t("strong",[_._v("训练新的树")]),_._v("：使用残差 "),t("smalltex",[_._v(" r_i^{(M)} ")]),_._v(" 来拟合一棵新的决策树 "),t("smalltex",[_._v(" h_M(x) ")]),_._v("：")],1),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nh_M(x) = \\arg\\min_h \\sum_{i=1}^{n} \\left( r_i^{(M)} - h(x_i) \\right)^2\n")])],1)]),_._v(" "),t("li",[t("p",[t("strong",[_._v("更新模型")]),_._v("：将新的树加到模型中：")]),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nF_M(x) = F_{M-1}(x) + \\eta h_M(x)\n")])],1),_._v(" "),t("p",[_._v("其中 "),t("smalltex",[_._v(" \\eta ")]),_._v(" 是学习率，控制步长。")],1)])]),_._v(" "),t("h3",{attrs:{id:"区别总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#区别总结"}},[_._v("#")]),_._v(" 区别总结")]),_._v(" "),t("ul",[t("li",[t("strong",[_._v("训练方式")]),_._v("：普通决策树是一棵树的独立构建，而GBDT是多棵树的累加，每棵树学习的是前一棵树的残差（梯度）。")]),_._v(" "),t("li",[t("strong",[_._v("目标函数")]),_._v("：普通决策树通过熵、基尼系数等指标直接分割数据，而GBDT通过最小化整体损失函数的梯度来优化模型。")]),_._v(" "),t("li",[t("strong",[_._v("模型复杂度")]),_._v("：GBDT是多个弱学习器（决策树）的组合，而普通决策树是单棵树。GBDT通过迭代优化损失，生成一个强大的集成模型。")])]),_._v(" "),t("h2",{attrs:{id:"bagging"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#bagging"}},[_._v("#")]),_._v(" Bagging")]),_._v(" "),t("h2",{attrs:{id:"lightgbm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#lightgbm"}},[_._v("#")]),_._v(" LightGBM")]),_._v(" "),t("h2",{attrs:{id:"xgboost"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#xgboost"}},[_._v("#")]),_._v(" XGBoost")]),_._v(" "),t("p",[_._v("在普通决策树中，基尼系数和熵计算的公式中用到的 "),t("smalltex",[_._v(" p_i ")]),_._v(" 表示的是某个类别 "),t("smalltex",[_._v(" i ")]),_._v(" 在节点中的概率，反映了某个类别在当前节点中样本的相对频率，用于评估节点的纯度或不确定性。")],1),_._v(" "),t("p",[_._v("逻辑回归\nSVM\n决策树、随机森林\n梯度决策树\nGBDT和")]),_._v(" "),t("ClientOnly",[t("leave")],1)],1)}),[],!1,null,null,null);t.default=l.exports}}]);