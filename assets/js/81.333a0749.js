(window.webpackJsonp=window.webpackJsonp||[]).push([[81],{447:function(t,e,v){"use strict";v.r(e);var l=v(3),_=Object(l.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"科研心得"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#科研心得"}},[t._v("#")]),t._v(" 科研心得")]),t._v(" "),e("ClientOnly",[e("title-pv")],1),t._v(" "),e("p",[t._v("此部分主要记录一些写给自己和未来的自己的科研心得，有来自于师兄师姐的教诲，也有个人科研过程中的感悟")]),t._v(" "),e("ul",[e("li",[e("p",[t._v("图神经网络的过平滑现象是指，随着图神经网络层数的变多，最终所有节点表示趋于一致，原理比较类似平稳随机过程的稳态，此时所有节点的表示不再是线性可分的，甚至会不如多层感知机的分类效果。也就是说，一个论文是什么类别，更多的是由他本身的内容是什么决定的，而不是由他引用了什么而决定的")])]),t._v(" "),e("li",[e("p",[t._v("注意力机制中，q和k乘积的计算结果，是一个 (n，n)的图，这个东西和邻接矩阵高度同构，如果你了解过图网络的消息传递你就会知道，kq矩阵与v的相乘，就是消息传递，而与图网络不一样的是，图网络的邻接矩阵是先验提供的，而transformer将靠模型自己去探索，也就是说，transformer是自己学习特征的权重图(关系图) ，然后进行消息传递加权")])]),t._v(" "),e("li",[e("p",[t._v("多头注意力的核心思想就是ensemble，如随机森林一样，将特征切分，每个head就像是一个弱分类器，让最后得到的embedding关注多方面信息，不要过拟合到某一种pattern上，这一点上面的实验图像可以很清晰的看出来。")])]),t._v(" "),e("li",[e("p",[t._v("在假设误差为高斯分布的情况下，最小化均方差损失函数与极大似然估计本质上是一致的。")])]),t._v(" "),e("li",[e("p",[t._v("(y'-y).detach()+y，解君愁")])]),t._v(" "),e("li",[e("p",[t._v("二元变量的互信息不可能为负数，因为已知一个变量不可能会对另一个变量带来负的信息增益，但是三元互信息是可以为负的，这表明了知道了第三个变量会对前两个变量的独立程度造成影响。")])]),t._v(" "),e("li",[e("p",[t._v("拉普拉斯算子在计算机视觉的意义是找到图像边缘")])]),t._v(" "),e("li",[e("p",[t._v("创新是问题驱动、任务驱动的，只有能提出一个好问题，才能做出好的创新")])]),t._v(" "),e("li",[e("p",[t._v("有时候审稿人并不care模型的效果，而关注的是你有没有提出一个好的问题，并用一个足够新颖的方法去解决。给出良好的直觉或解释要比性能上提高一两个点更重要")])]),t._v(" "),e("li",[e("p",[t._v("学术品味通常随着时间变化，上一个时代的好学术品味成了下一个时代的思维定式，正如牛顿不相信爱因斯坦的相对论，爱因斯坦不相信薛定谔的量子力学")])])]),t._v(" "),e("ClientOnly",[e("leave")],1)],1)}),[],!1,null,null,null);e.default=_.exports}}]);