(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{465:function(v,_,i){"use strict";i.r(_);var l=i(3),t=Object(l.a)({},(function(){var v=this,_=v._self._c;return _("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[_("h1",{attrs:{id:"大模型综合业务场景"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#大模型综合业务场景"}},[v._v("#")]),v._v(" 大模型综合业务场景")]),v._v(" "),_("ClientOnly",[_("title-pv")],1),v._v(" "),_("h2",{attrs:{id:"lora"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#lora"}},[v._v("#")]),v._v(" LoRA")]),v._v(" "),_("ul",[_("li",[v._v("LoRA的超参数有哪些？可训练的参数有哪些？")]),v._v(" "),_("li",[v._v("LongLoRA是什么？")]),v._v(" "),_("li",[v._v("LoRA的秩通常选为多少？")]),v._v(" "),_("li",[v._v("LoRA通常有两种实现方式，分别是怎么实现的？")]),v._v(" "),_("li",[v._v("QLoRA是什么？")]),v._v(" "),_("li",[v._v("QLoRA的nf4和fp32不能直接转换？为什么中间还有bf16？")]),v._v(" "),_("li",[v._v("讲一下混合精度训练的原理？")])]),v._v(" "),_("h2",{attrs:{id:"大模型"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#大模型"}},[v._v("#")]),v._v(" 大模型")]),v._v(" "),_("ul",[_("li",[v._v("扩充LLM上下文的方法")]),v._v(" "),_("li",[v._v("如何让大模型接受更长的文本？")]),v._v(" "),_("li",[v._v("长文本任务上，什么时候需要微调？什么时候不需要？")]),v._v(" "),_("li",[v._v("大模型如何判断该停止生成了？")]),v._v(" "),_("li",[v._v("灾难性遗忘的解决办法（领域知识训练后，通用知识遗忘解决办法）")]),v._v(" "),_("li",[v._v("大模型在SFT时是在学习什么")]),v._v(" "),_("li",[v._v("大模型的训练目标函数")]),v._v(" "),_("li",[v._v("大模型复读机问题的解决办法")]),v._v(" "),_("li",[v._v("大模型幻觉通常有哪些表现？\n"),_("ul",[_("li",[v._v("答非所问")]),v._v(" "),_("li",[v._v("遗忘上下文")]),v._v(" "),_("li",[v._v("偏离事实")])])]),v._v(" "),_("li",[v._v("大模型幻觉的产生原因\n"),_("ul",[_("li",[v._v("数据质量差")]),v._v(" "),_("li",[v._v("大模型高估了自己的能力，不知道问题边界")]),v._v(" "),_("li",[v._v("关键信息忽视")]),v._v(" "),_("li",[v._v("由于自回归的特性，如果一开始就错了，大模型之后只会将错就错")])])]),v._v(" "),_("li",[v._v("如何估算大模型训练或推理时，加载后要消耗多少显存？")]),v._v(" "),_("li",[v._v("大模型训练时瓶颈是在哪里？哪部分最耗时？")]),v._v(" "),_("li",[v._v("这些显存都用来存什么？")]),v._v(" "),_("li",[v._v("为什么大模型不擅长逻辑推理和逻辑计算")]),v._v(" "),_("li",[v._v("大模型训练时的最大困难\n"),_("ul",[_("li",[v._v("训练困难")]),v._v(" "),_("li",[v._v("语料污染")]),v._v(" "),_("li",[v._v("幻觉、灾难性遗忘")])])]),v._v(" "),_("li",[v._v("讲讲你对SFT和RLHF的理解")]),v._v(" "),_("li",[v._v("讲讲DPO、PPO的区别")]),v._v(" "),_("li",[v._v("Reward模型了解吗？")]),v._v(" "),_("li",[v._v("除了DPO还有哪些对齐算法？")]),v._v(" "),_("li",[v._v("LlaMA的分组查询注意力和Mistral有什么不同？")]),v._v(" "),_("li",[v._v("温度系数为什么可以影响输出?(还有topk、topp)")]),v._v(" "),_("li",[v._v("Deepspeed、Peft、vLLM是怎么实现数据并行、模型并行的？")]),v._v(" "),_("li",[v._v("MOE最开始时，各个专家结构和参数规模一样吗？")]),v._v(" "),_("li",[v._v("Self-Instruct的原理？")]),v._v(" "),_("li",[v._v("LLM的prompt构建技巧？")]),v._v(" "),_("li",[v._v("mmoe是什么？")]),v._v(" "),_("li",[v._v("PEFT微调和优缺点？")]),v._v(" "),_("li",[v._v("介绍一下InstructionGPT")]),v._v(" "),_("li",[v._v("普通模型的Query Fine-tune和SFT有什么区别？")]),v._v(" "),_("li",[v._v("GPT4o做了哪些优化加快了推理速度？")])]),v._v(" "),_("h2",{attrs:{id:"bert相关"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#bert相关"}},[v._v("#")]),v._v(" BERT相关")]),v._v(" "),_("ul",[_("li",[v._v("BERT、GPT训练时mask该怎么用？")]),v._v(" "),_("li",[v._v("GPT4对比GPT3.5的提升主要来自于哪些方面？")]),v._v(" "),_("li",[v._v("GPT3.5对比GPT3的提升主要来自于哪些方面？")]),v._v(" "),_("li",[v._v("为什么用BGE？有什么优势？")]),v._v(" "),_("li",[v._v("BERT和RoBERTa的区别？")]),v._v(" "),_("li",[v._v("BERT的几种Mask的作用是什么？")]),v._v(" "),_("li",[v._v("BERT预训练时的损失函数？")]),v._v(" "),_("li",[v._v("BERT和ELMo的区别？")]),v._v(" "),_("li",[v._v("文本特征提取器是什么？")]),v._v(" "),_("li",[v._v("BERT训练时的Worm-up")])]),v._v(" "),_("h2",{attrs:{id:"rag"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#rag"}},[v._v("#")]),v._v(" RAG")]),v._v(" "),_("ul",[_("li",[v._v("LLM知识如果和RAG冲突该怎么办")]),v._v(" "),_("li",[v._v("长文本LLM和RAG该怎么选择")]),v._v(" "),_("li",[v._v("RAG的准确率、召回率怎么计算？")]),v._v(" "),_("li",[v._v("困惑度怎么计算？")]),v._v(" "),_("li",[v._v("多路召回和重排序？")]),v._v(" "),_("li",[v._v("Faiss向量库的原理？")]),v._v(" "),_("li",[v._v("BLEU的缺点？")]),v._v(" "),_("li",[v._v("COS距离和欧氏距离是否同步增减？")])]),v._v(" "),_("h2",{attrs:{id:"深度学习"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#深度学习"}},[v._v("#")]),v._v(" 深度学习")]),v._v(" "),_("ul",[_("li",[v._v("PostNorm和PreNorm哪个更好？了解DeepNorm吗？")]),v._v(" "),_("li",[v._v("介绍一下 CLIP 模型？说说你认为 CLIP 为什么会这么强大？")]),v._v(" "),_("li",[v._v("聊一下你知道的推荐系统的深度学习模型？")]),v._v(" "),_("li",[v._v("LSTM、GRU的区别？")]),v._v(" "),_("li",[v._v("Word2vec是怎么来的？")]),v._v(" "),_("li",[v._v("讲讲Embedding的原理？这些模型通常是怎么训练的？")]),v._v(" "),_("li",[v._v("ddim和ddpm的原理？")]),v._v(" "),_("li",[v._v("SAM的原理？")]),v._v(" "),_("li",[v._v("激活函数的作用？")]),v._v(" "),_("li",[v._v("LayerNorm有哪些形式？")]),v._v(" "),_("li",[v._v("SENet和CBAM提出了什么算法？")])]),v._v(" "),_("h2",{attrs:{id:"机器学习"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#机器学习"}},[v._v("#")]),v._v(" 机器学习")]),v._v(" "),_("ul",[_("li",[v._v("DBSCAN和KMeans区别？")]),v._v(" "),_("li",[v._v("Gradient checkpoints 节省内存的原理是啥？")]),v._v(" "),_("li",[v._v("XGBoost只能用于数值的残差估计吗，可否用于特征？")]),v._v(" "),_("li",[v._v("XGBoost怎么填充缺失值？")]),v._v(" "),_("li",[v._v("随机森林如何保证每个树的随机性（数据、特征筛选，先随机筛选特征在取最优的）？")])]),v._v(" "),_("h2",{attrs:{id:"transformer"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[v._v("#")]),v._v(" Transformer")]),v._v(" "),_("ul",[_("li",[v._v("Softmax可以并行计算吗？")]),v._v(" "),_("li",[v._v("Softmax的指数上溢该怎么解决？")]),v._v(" "),_("li",[v._v("了解加法注意力吗？")]),v._v(" "),_("li",[v._v("讲讲Flash-Attention？")]),v._v(" "),_("li",[v._v("讲讲Paged-Attention？")]),v._v(" "),_("li",[v._v("Kv-cache是什么？")]),v._v(" "),_("li",[v._v("Sparse Attention是什么？")]),v._v(" "),_("li",[v._v("Transformer有哪些加速、防止过拟合的手段？")]),v._v(" "),_("li",[v._v("RoPE的虚部、实部有什么含义？")]),v._v(" "),_("li",[v._v("RoPE是加性编码还是乘性编码？")]),v._v(" "),_("li",[v._v("MLA怎么和RoPE结合？")]),v._v(" "),_("li",[v._v("绝对、相对编码的优势和不足？")]),v._v(" "),_("li",[v._v("Transformer和Llama的LN、FFN有什么区别")]),v._v(" "),_("li",[v._v("和Seq2Seq的区别\n"),_("ul",[_("li",[v._v("解码方法")]),v._v(" "),_("li",[v._v("Viterbi算法")])])]),v._v(" "),_("li",[v._v("多头注意力机制的好处？")]),v._v(" "),_("li",[v._v("分组注意力机制的好处？")]),v._v(" "),_("li",[v._v("Q，K，V的理解")]),v._v(" "),_("li",[v._v("除以平方根的目的")]),v._v(" "),_("li",[v._v("Sinusoidal、RoPE、ALiBi")]),v._v(" "),_("li",[v._v("是否了解长度外推与插值")]),v._v(" "),_("li",[v._v("Transformer的权重共享")]),v._v(" "),_("li",[v._v("Transformer的并行性")]),v._v(" "),_("li",[v._v("Transformer和RWKV、RNN、Informer")]),v._v(" "),_("li",[v._v("Transformers库的generate接口实现的repetition_penalty存在的问题")]),v._v(" "),_("li",[v._v("为什么初代GPT的性能比BERT差\n"),_("ul",[_("li",[v._v("GPT预训练时的任务更难（BERT的base就是为了和GPT对比，参数设定几乎一样）")]),v._v(" "),_("li",[v._v("BERT预训练用的数据集大小几乎是GPT的四倍")])])])]),v._v(" "),_("h2",{attrs:{id:"综合问题"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#综合问题"}},[v._v("#")]),v._v(" 综合问题")]),v._v(" "),_("ul",[_("li",[v._v("Scaling-Law是什么？")]),v._v(" "),_("li",[v._v("Loss Spike问题的解决办法？")]),v._v(" "),_("li",[v._v("如何把英文大模型转为中文大模型（领域词表扩建、预训练、SFT）")]),v._v(" "),_("li",[v._v("想让模型适应新领域，应该预训练还是微调还是RAG")]),v._v(" "),_("li",[v._v("涌现的原因")]),v._v(" "),_("li",[v._v("前馈层在不同位置的作用？")]),v._v(" "),_("li",[v._v("对多模态模型的理解")]),v._v(" "),_("li",[v._v("多轮对话如何微调？")]),v._v(" "),_("li",[v._v("如何解决多轮对话遗忘前面对话的问题？")]),v._v(" "),_("li",[v._v("CLS等标记的作用")]),v._v(" "),_("li",[v._v("Llama和GPT预训练时有什么区别？")]),v._v(" "),_("li",[v._v("样本不均衡时该怎么办？")]),v._v(" "),_("li",[v._v("Focal loss是什么？")])]),v._v(" "),_("h2",{attrs:{id:"手撕代码"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#手撕代码"}},[v._v("#")]),v._v(" 手撕代码")]),v._v(" "),_("ul",[_("li",[v._v("假设给你两个链表，如何找到公共节点，只能是Y字型？")]),v._v(" "),_("li",[v._v("元组的元素必须是不可变吗？")]),v._v(" "),_("li",[v._v("讲讲堆排序、优先队列的思想")]),v._v(" "),_("li",[v._v("什么是稳定排序？")]),v._v(" "),_("li",[v._v("平衡二叉树的原理？")]),v._v(" "),_("li",[v._v("如何找到一个数组的中位数？（复杂度N，分治法）")]),v._v(" "),_("li",[v._v("打家劫舍")]),v._v(" "),_("li",[v._v("矩阵置零")]),v._v(" "),_("li",[v._v("跳跃游戏")]),v._v(" "),_("li",[v._v("Shuffle数组")]),v._v(" "),_("li",[v._v("手撕BERT+LoRA")]),v._v(" "),_("li",[v._v("ndcg、auc、roc怎么算？")]),v._v(" "),_("li",[v._v("Louvain算法的核心思想？还知道哪些社区算法？")])]),v._v(" "),_("h2",{attrs:{id:"自由问题"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#自由问题"}},[v._v("#")]),v._v(" 自由问题")]),v._v(" "),_("ul",[_("li",[v._v("对o1、o1-mini是怎么看的？")]),v._v(" "),_("li",[v._v("你认为明年大模型发展的趋势？")]),v._v(" "),_("li",[v._v("你怎么看待我们的团队、公司？有哪些好的、坏的点？")]),v._v(" "),_("li",[v._v("为什么去年openai的chatgpt成功了，但是gpt2和instructgpt没有？")])]),v._v(" "),_("ClientOnly",[_("leave")],1)],1)}),[],!1,null,null,null);_.default=t.exports}}]);