(window.webpackJsonp=window.webpackJsonp||[]).push([[64],{431:function(a,t,r){"use strict";r.r(t);var s=r(3),e=Object(s.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"大模型训练"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型训练"}},[a._v("#")]),a._v(" 大模型训练")]),a._v(" "),t("ClientOnly",[t("title-pv")],1),a._v(" "),t("h2",{attrs:{id:"数据准备"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据准备"}},[a._v("#")]),a._v(" 数据准备")]),a._v(" "),t("p",[a._v("数据通常分为通用数据，如网页、书籍、对话；以及专业数据，如多语言数据、科学文本（学术论文）、代码等等")]),a._v(" "),t("h2",{attrs:{id:"数据清洗"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据清洗"}},[a._v("#")]),a._v(" 数据清洗")]),a._v(" "),t("p",[a._v("为保证大模型使用高质量语料训练，通常需要质量过滤，包括冗余去除、隐私消除、词元切分等步骤")]),a._v(" "),t("h2",{attrs:{id:"数据选取、划分"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据选取、划分"}},[a._v("#")]),a._v(" 数据选取、划分")]),a._v(" "),t("h3",{attrs:{id:"scaling-law"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#scaling-law"}},[a._v("#")]),a._v(" Scaling Law")]),a._v(" "),t("p",[a._v("对于计算量"),t("smalltex",[a._v("C")]),a._v("，模型参数量"),t("smalltex",[a._v("N")]),a._v("，和数据集大小"),t("smalltex",[a._v("D")]),a._v("，当不受其他两个因素制约时，模型性能"),t("smalltex",[a._v("L")]),a._v("与每个因素都呈现幂律关系，且随着每个因素的量增大，对模型的效果提升越有限。目前已经有研究证明，模型参数量上升时，数据量也需要等比例上升。")],1),a._v(" "),t("h3",{attrs:{id:"数据选取方法"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#数据选取方法"}},[a._v("#")]),a._v(" 数据选取方法")]),a._v(" "),t("ul",[t("li",[a._v("Data Mixing Laws：通过在小规模数据和模型上进行实验，利用训练步数、模型大小和数据混合比例的缩放定律（Scaling Laws），来预测在大规模数据和大型模型上的性能")]),a._v(" "),t("li",[a._v("DoGE/LESS：重点学习对整体梯度贡献较大的领域，可以使用影响函数来量化每条训练数据对模型的影响。但通常在大模型上直接计算影响函数是困难的，因此可以先知识蒸馏，在小模型上用影响函数分析什么样的数据是关键的")]),a._v(" "),t("li",[a._v("REGMIX：使用多种数据配比训练一组小型模型，并拟合一个回归模型来预测给定各自配比的模型的性能")])]),a._v(" "),t("h2",{attrs:{id:"训练过程"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#训练过程"}},[a._v("#")]),a._v(" 训练过程")]),a._v(" "),t("h3",{attrs:{id:"pre-train"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pre-train"}},[a._v("#")]),a._v(" Pre-train")]),a._v(" "),t("h3",{attrs:{id:"sft"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sft"}},[a._v("#")]),a._v(" SFT")]),a._v(" "),t("h3",{attrs:{id:"rlhf"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#rlhf"}},[a._v("#")]),a._v(" RLHF")]),a._v(" "),t("h2",{attrs:{id:"ppo、dpo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ppo、dpo"}},[a._v("#")]),a._v(" PPO、DPO")]),a._v(" "),t("h3",{attrs:{id:"ppo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#ppo"}},[a._v("#")]),a._v(" PPO")]),a._v(" "),t("ul",[t("li",[a._v("Reward Model")])]),a._v(" "),t("h3",{attrs:{id:"dpo"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#dpo"}},[a._v("#")]),a._v(" DPO")]),a._v(" "),t("h3",{attrs:{id:"两者的优势与不足"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#两者的优势与不足"}},[a._v("#")]),a._v(" 两者的优势与不足")]),a._v(" "),t("h2",{attrs:{id:"微调技术"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#微调技术"}},[a._v("#")]),a._v(" 微调技术")]),a._v(" "),t("h3",{attrs:{id:"adapter-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#adapter-tuning"}},[a._v("#")]),a._v(" Adapter tuning")]),a._v(" "),t("h3",{attrs:{id:"prompt-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prompt-tuning"}},[a._v("#")]),a._v(" Prompt tuning")]),a._v(" "),t("h3",{attrs:{id:"prefix-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#prefix-tuning"}},[a._v("#")]),a._v(" Prefix tuning")]),a._v(" "),t("h3",{attrs:{id:"fine-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning"}},[a._v("#")]),a._v(" Fine tuning")]),a._v(" "),t("h3",{attrs:{id:"p-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#p-tuning"}},[a._v("#")]),a._v(" P-tuning")]),a._v(" "),t("h3",{attrs:{id:"freeze-tuning"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#freeze-tuning"}},[a._v("#")]),a._v(" Freeze tuning")]),a._v(" "),t("h3",{attrs:{id:"lora"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#lora"}},[a._v("#")]),a._v(" LoRA")]),a._v(" "),t("h3",{attrs:{id:"qlora"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#qlora"}},[a._v("#")]),a._v(" QLoRA")]),a._v(" "),t("h2",{attrs:{id:"tokenizer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#tokenizer"}},[a._v("#")]),a._v(" Tokenizer")]),a._v(" "),t("h3",{attrs:{id:"byte-pair-encoding-bpe"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#byte-pair-encoding-bpe"}},[a._v("#")]),a._v(" Byte-Pair Encoding(BPE)")]),a._v(" "),t("h3",{attrs:{id:"wordpiece"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#wordpiece"}},[a._v("#")]),a._v(" WordPiece")]),a._v(" "),t("h3",{attrs:{id:"sentencepiece"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#sentencepiece"}},[a._v("#")]),a._v(" SentencePiece")]),a._v(" "),t("h3",{attrs:{id:"unigram"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#unigram"}},[a._v("#")]),a._v(" Unigram")]),a._v(" "),t("h3",{attrs:{id:"词汇表不全、过大问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#词汇表不全、过大问题"}},[a._v("#")]),a._v(" 词汇表不全、过大问题")]),a._v(" "),t("h3",{attrs:{id:"各路llm的tokenizer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#各路llm的tokenizer"}},[a._v("#")]),a._v(" 各路LLM的Tokenizer")]),a._v(" "),t("h2",{attrs:{id:"训练损失函数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#训练损失函数"}},[a._v("#")]),a._v(" 训练损失函数")]),a._v(" "),t("h3",{attrs:{id:"bert"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#bert"}},[a._v("#")]),a._v(" BERT")]),a._v(" "),t("ul",[t("li",[a._v("MLM(Masked Language Model)：在MLM任务中掩盖部分词汇，并通过最大化掩盖词汇的预测概率来进行训练，假设"),t("smalltex",[a._v("T")]),a._v("是被掩盖的词语集合，"),t("smalltex",[a._v("p_\\theta(w_t|C)")]),a._v("是模型在上下文"),t("smalltex",[a._v("C")]),a._v("下预测词汇"),t("smalltex",[a._v("w_t")]),a._v("的概率，那么MLM损失为"),t("smalltex",[a._v("L_{MLM}=-\\Sigma_{t \\in T}log p_\\theta(w_t|C)")])],1),a._v(" "),t("li",[a._v("GPT")])]),a._v(" "),t("h3",{attrs:{id:"gpt"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gpt"}},[a._v("#")]),a._v(" GPT")]),a._v(" "),t("ClientOnly",[t("leave")],1)],1)}),[],!1,null,null,null);t.default=e.exports}}]);