(window.webpackJsonp=window.webpackJsonp||[]).push([[103],{470:function(_,t,v){"use strict";v.r(t);var a=v(3),i=Object(a.a)({},(function(){var _=this,t=_._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":_.$parent.slotKey}},[t("h1",{attrs:{id:"机器学习基础"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#机器学习基础"}},[_._v("#")]),_._v(" 机器学习基础")]),_._v(" "),t("ClientOnly",[t("title-pv")],1),_._v(" "),t("h2",{attrs:{id:"决策树"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#决策树"}},[_._v("#")]),_._v(" 决策树")]),_._v(" "),t("p",[_._v("普通决策树（如CART决策树）和基于梯度的决策树（如梯度提升决策树，GBDT）的主要区别体现在模型构建方式和目标优化方法上。在普通决策树中，基尼系数和熵计算的公式中用到的 "),t("smalltex",[_._v(" p_i ")]),_._v(" 表示的是某个类别 "),t("smalltex",[_._v(" i ")]),_._v(" 在节点中的概率，反映了某个类别在当前节点中样本的相对频率，用于评估节点的纯度或不确定性。")],1),_._v(" "),t("h3",{attrs:{id:"普通决策树-cart"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#普通决策树-cart"}},[_._v("#")]),_._v(" 普通决策树（CART）")]),_._v(" "),t("p",[_._v("普通决策树递归地对数据进行划分，基于某种标准（如基尼系数或信息增益）选择特征和分裂点构建的。其目标是最大化子节点的纯度。")]),_._v(" "),t("p",[t("strong",[_._v("决策过程：")])]),_._v(" "),t("ul",[t("li",[t("p",[_._v("对于每个节点，决策树尝试找到最优特征 "),t("smalltex",[_._v(" j ")]),_._v(" 和分裂点 "),t("smalltex",[_._v(" s ")]),_._v("，使得分裂后的损失函数（如基尼系数或熵）最小化。")],1),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\n\\min_{j, s} \\left( L(\\text{左子节点}) + L(\\text{右子节点}) \\right)\n")])],1)])]),_._v(" "),t("p",[_._v("其中，损失函数 "),t("smalltex",[_._v(" L ")]),_._v(" 可能是基尼系数 "),t("smalltex",[_._v(" G ")]),_._v(" 或熵 "),t("smalltex",[_._v(" H ")]),_._v("：")],1),_._v(" "),t("ul",[t("li",[_._v("基尼系数："),t("smalltex",[_._v(" G(p) = 1 - \\sum_{i=1}^{k} p_i^2 ")])],1),_._v(" "),t("li",[_._v("熵："),t("smalltex",[_._v(" H(p) = -\\sum_{i=1}^{k} p_i \\log(p_i) ")])],1)]),_._v(" "),t("p",[_._v("树的构建是贪心的，每次选取当前最优分裂点。")]),_._v(" "),t("h3",{attrs:{id:"回归树"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#回归树"}},[_._v("#")]),_._v(" 回归树")]),_._v(" "),t("p",[_._v("回归树会尝试通过“年龄”变量来分割数据集，确保每个分割后的子集的平均收入差异最小（即最小化均方误差）")]),_._v(" "),t("h3",{attrs:{id:"随机森林"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#随机森林"}},[_._v("#")]),_._v(" 随机森林")]),_._v(" "),t("p",[_._v("随机森林是一种集成学习算法，基于决策树或回归树构建。它通过构建多个决策树或回归树，并通过投票或平均值来提高模型的稳定性和准确性。随机森林的核心思想是通过引入随机性来降低单棵树的过拟合现象，并增强模型的泛化能力。")]),_._v(" "),t("h2",{attrs:{id:"梯度决策树-gbdt"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#梯度决策树-gbdt"}},[_._v("#")]),_._v(" 梯度决策树（GBDT）")]),_._v(" "),t("p",[_._v("GBDT 是一种集成模型，通过不断迭代训练多个决策树来优化模型性能。与普通决策树不同，GBDT的每棵树是在前一棵树的预测残差（误差）的基础上构建的，目的是通过迭代地最小化残差来提高预测精度。")]),_._v(" "),t("p",[_._v("需要注意的是，GBDT并不是真正的用决策树预测结果，而是用决策树作为梯度指导去优化一个模型，使模型能达到更好的性能。")]),_._v(" "),t("p",[_._v("GBDT的目标是最小化整体损失函数：")]),_._v(" "),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nL = \\sum_{i=1}^{n} \\ell(y_i, F_M(x_i))\n")])],1),_._v(" "),t("p",[_._v("其中：")]),_._v(" "),t("ul",[t("li",[t("smalltex",[_._v(" y_i ")]),_._v(" 是实际标签，"),t("smalltex",[_._v(" F_M(x_i) ")]),_._v(" 是第 "),t("smalltex",[_._v(" M ")]),_._v(" 轮模型的预测值。")],1),_._v(" "),t("li",[t("smalltex",[_._v(" \\ell(y_i, F(x_i)) ")]),_._v(" 是损失函数，常见的是均方误差（MSE）："),t("smalltex",[_._v(" \\ell(y_i, F(x_i)) = (y_i - F(x_i))^2 ")]),_._v("。")],1),_._v(" "),t("li",[t("smalltex",[_._v(" F_M(x_i) = F_{M-1}(x_i) + \\eta \\cdot h_M(x_i) ")]),_._v("，其中 "),t("smalltex",[_._v(" h_M(x_i) ")]),_._v(" 是第 "),t("smalltex",[_._v(" M ")]),_._v(" 棵树，"),t("smalltex",[_._v(" \\eta ")]),_._v(" 是学习率。")],1)]),_._v(" "),t("p",[t("strong",[_._v("梯度提升步骤：")])]),_._v(" "),t("ol",[t("li",[t("p",[t("strong",[_._v("初始化模型")]),_._v("：首先初始化一个常数模型，通常为目标变量的均值。\n")]),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nF_0(x) = \\arg\\min_{c} \\sum_{i=1}^{n} \\ell(y_i, c)\n")])],1),t("p")]),_._v(" "),t("li",[t("p",[t("strong",[_._v("计算残差")]),_._v("：在第 "),t("smalltex",[_._v(" M ")]),_._v(" 步，计算前一轮模型的残差（即当前的负梯度）：\n")],1),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nr_i^{(M)} = - \\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F(x_i)}\\right]"),t("em",[_._v("{F(x_i) = F")]),_._v("{M-1}(x_i)}\n")])],1),t("p")]),_._v(" "),t("li",[t("p",[t("strong",[_._v("训练新的树")]),_._v("：使用残差 "),t("smalltex",[_._v(" r_i^{(M)} ")]),_._v(" 来拟合一棵新的决策树 "),t("smalltex",[_._v(" h_M(x) ")]),_._v("：\n")],1),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nh_M(x) = \\arg\\min_h \\sum_{i=1}^{n} \\left( r_i^{(M)} - h(x_i) \\right)^2\n")])],1),t("p")]),_._v(" "),t("li",[t("p",[t("strong",[_._v("更新模型")]),_._v("：将新的树加到模型中：\n")]),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nF_M(x) = F_{M-1}(x) + \\eta h_M(x)\n")])],1),_._v("\n其中 "),t("smalltex",[_._v(" \\eta ")]),_._v(" 是学习率，控制步长。"),t("p")],1)]),_._v(" "),t("h4",{attrs:{id:"区别总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#区别总结"}},[_._v("#")]),_._v(" 区别总结")]),_._v(" "),t("ul",[t("li",[t("strong",[_._v("训练方式")]),_._v("：普通决策树是一棵树的独立构建，而GBDT是多棵树的累加，每棵树学习的是前一棵树的残差（梯度）。")]),_._v(" "),t("li",[t("strong",[_._v("目标函数")]),_._v("：普通决策树通过熵、基尼系数等指标直接分割数据，而GBDT通过最小化整体损失函数的梯度来优化模型。")]),_._v(" "),t("li",[t("strong",[_._v("模型复杂度")]),_._v("：GBDT是多个弱学习器（决策树）的组合，而普通决策树是单棵树。GBDT通过迭代优化损失，生成一个强大的集成模型。")])]),_._v(" "),t("h3",{attrs:{id:"bagging-bootstrap-aggregating"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#bagging-bootstrap-aggregating"}},[_._v("#")]),_._v(" Bagging (Bootstrap Aggregating)")]),_._v(" "),t("p",[_._v("公式：\n")]),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nf(x) = \\frac{1}{n} \\sum_{i=1}^{n} T_i(x)\n")])],1),t("p"),_._v(" "),t("ul",[t("li",[t("strong",[_._v("核心思想")]),_._v(": 对数据集进行多次重采样，分别训练多个决策树模型 "),t("smalltex",[_._v("T_i")]),_._v("，最后通过平均或投票的方式进行预测。")],1),_._v(" "),t("li",[t("strong",[_._v("适用场景")]),_._v(": 当基础模型的方差较大时（易过拟合），Bagging 能通过并行训练和投票减少方差。适合随机森林（Random Forest）等。")]),_._v(" "),t("li",[t("strong",[_._v("优点")]),_._v(": 减少模型方差，适合数据量较大且无明显噪声的数据。")])]),_._v(" "),t("h3",{attrs:{id:"lightgbm-light-gradient-boosting-machine"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#lightgbm-light-gradient-boosting-machine"}},[_._v("#")]),_._v(" LightGBM (Light Gradient Boosting Machine)")]),_._v(" "),t("p",[_._v("公式：\n")]),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nf(x) = \\sum_{i=1}^{N} \\alpha_i T_i(x)\n")])],1),t("p"),_._v(" "),t("ul",[t("li",[t("strong",[_._v("核心思想")]),_._v(": 基于梯度提升决策树（Gradient Boosting Decision Tree, GBDT），采用 "),t("strong",[_._v("叶子节点增长策略")]),_._v("（Leaf-wise Growth）和 "),t("strong",[_._v("基于直方图的快速训练")]),_._v(" 方法，通过梯度信息逐步优化决策树。")]),_._v(" "),t("li",[t("strong",[_._v("适用场景")]),_._v(": 当数据量较大且特征维度较高时（尤其是稀疏数据），LightGBM 在速度和内存使用上有优势。")]),_._v(" "),t("li",[t("strong",[_._v("优点")]),_._v(": 训练速度快，适合大规模、高维稀疏数据，且处理类别型特征效果较好。")])]),_._v(" "),t("h3",{attrs:{id:"xgboost-extreme-gradient-boosting"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#xgboost-extreme-gradient-boosting"}},[_._v("#")]),_._v(" XGBoost (Extreme Gradient Boosting)")]),_._v(" "),t("p",[_._v("公式：\n")]),t("div",{staticStyle:{"text-align":"center"}},[t("tex",[_._v("\nf(x) = \\sum_{i=1}^{N} \\alpha_i T_i(x) + \\lambda \\sum_{i=1}^{N} \\Omega(T_i)\n")])],1),t("p"),_._v(" "),t("p",[_._v("其中，"),t("smalltex",[_._v("\\Omega(T_i)")]),_._v(" 是树的复杂度惩罚项，"),t("smalltex",[_._v("\\lambda")]),_._v(" 是正则化系数。")],1),_._v(" "),t("ul",[t("li",[t("strong",[_._v("核心思想")]),_._v(": 基于梯度提升框架的加速版本，强调 "),t("strong",[_._v("正则化")]),_._v(" 和 "),t("strong",[_._v("损失函数的优化")]),_._v("，防止过拟合。使用逐步构建树的方式，优化残差。")]),_._v(" "),t("li",[t("strong",[_._v("适用场景")]),_._v(": 当模型容易过拟合时，XGBoost 提供强大的正则化能力。适合复杂特征和需要精度高的场景（如比赛、精准预测）。")]),_._v(" "),t("li",[t("strong",[_._v("优点")]),_._v(": 性能优化较好，适合处理缺失数据和噪声较大的数据，具有强大的正则化机制防止过拟合。")])]),_._v(" "),t("hr"),_._v(" "),t("h4",{attrs:{id:"总结"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[_._v("#")]),_._v(" 总结")]),_._v(" "),t("ul",[t("li",[t("strong",[_._v("Bagging")]),_._v("：并行训练多个决策树，适合大数据且易过拟合的场景。")]),_._v(" "),t("li",[t("strong",[_._v("LightGBM")]),_._v("：基于梯度提升的决策树，适合高维稀疏数据，速度快。")]),_._v(" "),t("li",[t("strong",[_._v("XGBoost")]),_._v("：正则化加强的梯度提升树，适合需要高精度和防过拟合的场景。")])]),_._v(" "),t("p",[_._v("交叉熵误差公式\nKL散度公式\n逻辑回归\nSVM")]),_._v(" "),t("ClientOnly",[t("leave")],1)],1)}),[],!1,null,null,null);t.default=i.exports}}]);