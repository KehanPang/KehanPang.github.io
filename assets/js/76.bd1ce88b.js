(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{468:function(n,t,e){"use strict";e.r(t);var s=e(3),l=Object(s.a)({},(function(){var n=this,t=n._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[t("h1",{attrs:{id:"大模型综合业务场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型综合业务场景"}},[n._v("#")]),n._v(" 大模型综合业务场景")]),n._v(" "),t("ClientOnly",[t("title-pv")],1),n._v(" "),t("p",[n._v("Scaling-Law\nLoss Spike问题的解决办法\n如何让大模型接受更长的文本\n如何判断大模型该停止生成了\n如何把英文大模型转为中文大模型\n灾难性遗忘的解决办法\n领域知识训练后，通用知识遗忘解决办法\n领域词表扩建\n想让模型适应新领域，应该预训练还是微调还是RAG\n大模型在SFT时是在学习什么\n大模型的训练目标函数\n大模型复读机问题的解决办法\n幻觉的产生原因\n数据质量\n大模型高估了自己的能力，不知道问题边界\n对齐问题\n将错就错")]),n._v(" "),t("p",[n._v("大模型幻觉的解决办法\n答非所问\n遗忘上下文\n偏离事实")]),n._v(" "),t("p",[n._v("涌现的原因\n不同大小的模型的训练时、推理时显存占用\n前馈层在不同位置的作用\n对多模态模型的理解\n为什么大模型不擅长逻辑推理和逻辑计算\n多轮对话微调\n训LLM最大困难")]),n._v(" "),t("ClientOnly",[t("leave")],1)],1)}),[],!1,null,null,null);t.default=l.exports}}]);