(window.webpackJsonp=window.webpackJsonp||[]).push([[77],{476:function(n,r,e){"use strict";e.r(r);var t=e(3),a=Object(t.a)({},(function(){var n=this,r=n._self._c;return r("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[r("h1",{attrs:{id:"transformer"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[n._v("#")]),n._v(" Transformer")]),n._v(" "),r("ClientOnly",[r("title-pv")],1),n._v(" "),r("h2",{attrs:{id:"手撕transformer代码"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#手撕transformer代码"}},[n._v("#")]),n._v(" 手撕Transformer代码")]),n._v(" "),r("p",[n._v("Transformer和Llama的LN、FFN有什么区别\n和Seq2Seq的区别\n解码方法\nViterbi算法")]),n._v(" "),r("p",[n._v("多头注意力\nQ，K，V的理解\n除以平方根的目的\n位置编码与旋转编码\nSinusoidal、RoPE、ALiBi\n绝对编码与相对编码\n长度外推与插值\n两者的优劣\nALiBi (Attention with Linear Biases)")]),n._v(" "),r("p",[n._v("自注意力机制\n掩码注意力机制\nTransformer的权重共享\nTransformer的并行性\nTransformer和RWKV、RNN、Informer\nTransformers库的generate接口实现的repetition_penalty存在的问题\nBERT和RoBerta的区别\nBERT和GPT的区别\nBERT的双向体现在哪里\nBERT是如何训练的\nBERT如何解决长文本问题\nBERT训练时的Worm-up\n为什么初代GPT的性能比BERT差\nGPT预训练时的任务更难（BERT的base就是为了和GPT对比，参数设定几乎一样）\nBERT预训练用的数据集大小几乎是GPT的四倍")]),n._v(" "),r("p",[n._v("GPT4o做了哪些优化加快了推理速度？")]),n._v(" "),r("ClientOnly",[r("leave")],1)],1)}),[],!1,null,null,null);r.default=a.exports}}]);