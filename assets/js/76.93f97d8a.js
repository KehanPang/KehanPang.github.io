(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{460:function(n,t,e){"use strict";e.r(t);var o=e(3),s=Object(o.a)({},(function(){var n=this,t=n._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[t("h1",{attrs:{id:"大模型综合业务场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型综合业务场景"}},[n._v("#")]),n._v(" 大模型综合业务场景")]),n._v(" "),t("ClientOnly",[t("title-pv")],1),n._v(" "),t("p",[n._v("LoRA超参数有什么，可训练的参数有什么\nScaling-Law\nLoss Spike问题的解决办法\n如何让大模型接受更长的文本\n如何判断大模型该停止生成了\nLLM知识如果和RAG冲突该怎么办\n长文本LLM、RAG\n扩充LLM上下文的方法\n如何把英文大模型转为中文大模型（词表适配、预训练、SFT）\n灾难性遗忘的解决办法\n领域知识训练后，通用知识遗忘解决办法\n领域词表扩建\n想让模型适应新领域，应该预训练还是微调还是RAG\n大模型在SFT时是在学习什么\n大模型的训练目标函数\n大模型复读机问题的解决办法\n幻觉的产生原因\n数据质量\n大模型高估了自己的能力，不知道问题边界\n对齐问题\n将错就错")]),n._v(" "),t("p",[n._v("大模型幻觉的解决办法\n答非所问\n遗忘上下文\n偏离事实")]),n._v(" "),t("p",[n._v("涌现的原因\n不同大小的模型的训练时、推理时显存占用\n前馈层在不同位置的作用\n对多模态模型的理解\n为什么大模型不擅长逻辑推理和逻辑计算\n多轮对话如何微调？\n如何解决多轮对话遗忘前面对话的问题？\n训LLM最大困难\nCLS等标记的作用\nLlama和GPT预训练时有什么区别？\n样本不均衡时该怎么办？\nFocal loss是什么？\nBERT、GPT训练时mask该怎么用？\nGPT4对比GPT3.5的提升主要来自于哪些方面？\nGPT3.5对比GPT3的提升主要来自于哪些方面？\n讲讲你对SFT和RLHF的理解\n困惑度怎么计算？\nRAG的准确率、召回率怎么计算？\nPostNorm和PreNorm哪个更好？了解DeepNorm吗？\nSoftmax可以并行计算吗？\nSoftmax的指数上溢该怎么解决？\n了解加法注意力吗？\nSENet和CBAM提出了什么算法？\nDBSCAN和KMeans区别？\ngradient checkpoints 节省内存的原理是啥？\n讲一下混合精度训练的原理？\n介绍一下 CLIP 模型？说说你认为 CLIP 为什么会这么强大？\n为什么用BGE？\nXGBoost只能用于数值的残差估计吗，可否用于特征？\nXGBoost怎么填充缺失值？\n随机森林如何保证每个树的随机性（数据、特征筛选，先随机筛选特征在取最优的）？\n聊一下你知道的推荐系统的深度学习模型？\n假设给你两个链表，如何找到公共节点，只能是Y字型的？\n如何找到一个数组的中位数？（复杂度N，分治法）\n多路召回和重排序？\n大模型存在的问题？")]),n._v(" "),t("ul",[t("li",[n._v("训练困难")]),n._v(" "),t("li",[n._v("语料污染")]),n._v(" "),t("li",[n._v("幻觉、灾难性遗忘\n讲讲DPO、PPO的区别\nReward模型了解吗？\nWord2vec是怎么来的？\n讲讲Embedding的原理？\n如何扩展长文本？\n你认为明年大模型发展的趋势？\n你怎么看待我们的团队、公司？好的、坏的点？\n大模型训练时瓶颈是在哪里？哪部分最好耗时？\n长文本任务上，什么时候需要微调？什么时候不需要？\nddim和ddpm的原理？\nSAM的原理？\nLLM的prompt构建技巧？\nFaiss向量库的原理？\n讲讲LongLoRA\nSparse Attention是什么？\nTransformer有哪些加速、防止过拟合的手段？")])]),n._v(" "),t("ClientOnly",[t("leave")],1)],1)}),[],!1,null,null,null);t.default=s.exports}}]);