(window.webpackJsonp=window.webpackJsonp||[]).push([[76],{440:function(n,t,e){"use strict";e.r(t);var s=e(3),a=Object(s.a)({},(function(){var n=this,t=n._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":n.$parent.slotKey}},[t("h1",{attrs:{id:"大模型综合业务场景"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大模型综合业务场景"}},[n._v("#")]),n._v(" 大模型综合业务场景")]),n._v(" "),t("ClientOnly",[t("title-pv")],1),n._v(" "),t("p",[n._v("LoRA超参数有什么，可训练的参数有什么\nScaling-Law\nLoss Spike问题的解决办法\n如何让大模型接受更长的文本\n如何判断大模型该停止生成了\nLLM知识如果和RAG冲突该怎么办\n长文本LLM、RAG\n扩充LLM上下文的方法\n如何把英文大模型转为中文大模型（词表适配、预训练、SFT）\n灾难性遗忘的解决办法\n领域知识训练后，通用知识遗忘解决办法\n领域词表扩建\n想让模型适应新领域，应该预训练还是微调还是RAG\n大模型在SFT时是在学习什么\n大模型的训练目标函数\n大模型复读机问题的解决办法\n幻觉的产生原因\n数据质量\n大模型高估了自己的能力，不知道问题边界\n对齐问题\n将错就错")]),n._v(" "),t("p",[n._v("大模型幻觉的解决办法\n答非所问\n遗忘上下文\n偏离事实")]),n._v(" "),t("p",[n._v("涌现的原因\n不同大小的模型的训练时、推理时显存占用\n前馈层在不同位置的作用\n对多模态模型的理解\n为什么大模型不擅长逻辑推理和逻辑计算\n多轮对话如何微调？\n如何解决多轮对话遗忘前面对话的问题？\n训LLM最大困难\nCLS等标记的作用\nLlama和GPT预训练时有什么区别？\n样本不均衡时该怎么办？\nFocal loss是什么？\nBERT、GPT训练时mask该怎么用？\nGPT4对比GPT3.5的提升主要来自于哪些方面？\nGPT3.5对比GPT3的提升主要来自于哪些方面？\n讲讲你对SFT和RLHF的理解\n困惑度怎么计算？\nRAG的准确率、召回率怎么计算？\nPostNorm和PreNorm哪个更好？了解DeepNorm吗？")]),n._v(" "),t("ClientOnly",[t("leave")],1)],1)}),[],!1,null,null,null);t.default=a.exports}}]);